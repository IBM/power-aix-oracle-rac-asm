---
# Variable file

# Variables that need updates are tagged with "# !!! update !!!".
# Although the default values would work, they may not be your preference
# or following your organization's convention. Review the defaults and make
# updates as appropriate.
# NOTE: updates can be made in this file or put the updated variables in 
#       <role>/vars/main.yml to override the defaults here. Remember
#       variables are defined in a namespace starting from <role>.
#       db_asmadmingroup value will be set for oracle binary and shared asm disks

###############################################################################
global:
###############################################################################

  # !!! update !!!
  root_password: "XXXXYYYYZZZZ"
  grid_owner:  &grid_owner  "grid"
  grid_pgroup: &grid_pgroup "oinstall"
  db_owner:    &db_owner    "oracle"
  db_pgroup:   &db_pgroup   "oinstall"
  db_asmadmingroup: &db_asmadmingroup  "asmadmin"
  db_asmdbagroup: &db_asmdbagroup  "asmdba"
  # db_pgroup is the primary group for database user, it should match
  # config.grid_rsp.install.asm.OSASM
  # grid_pgroup is the primary group for grid user
  work_dir:   &work_dir "/tmp/ansible"
  # Do not change the following 4 variables and their values
  scripts_dir: [ *work_dir, "scripts" ]
  saved_dir:   [ *work_dir, "saved" ]
  done_dir:    [ *work_dir, "done" ]
  files_dir:   [ *work_dir, "files" ]

###############################################################################
bootstrap:
###############################################################################

  download_dir: "~"
  rpm_rte:     "rpm.rte"
  yum_bundle:  "yum_bundle.tar"

  netsvc_conf: [ "hosts = local, bind4" ]

  resolv_conf:
    options:  "timeout:1"
    # Order of nameservers is preserved.
    # The first list element is for resolving Oracle VIPs and SCAN name. If the first
    # element does not resolve DNS addresses for downloading RPMs during
    # installation, the second, third etc. can be specified for that.
    # The second, third etc. elements will be commented in /etc/resolv.conf
    # before starting gridSetup.sh/runInstaller.sh becaue Oracle does not allow
    # more than one nameservers. After gridSetup.sh/runInstaller complete,
    # the commented out entries will be restored.
    # !!! update !!!
    nameservers: [ "192.168.10.1","9.40.16.1" ]
    # !!! update !!!
    search:   "example.com"

###############################################################################
preconfig:    
###############################################################################

  # Set timezone
  # See /usr/share/lib/zoneinfo/ for a list of availble values.
  # https://www.ibm.com/support/pages/managing-time-zone-variable-posix
  # !!! update !!!
  tz: "America/Los_Angeles"

  # Configure NTP and aix_timeserver are mutually exclusive.

  # Add AIX network timeserver to /etc/hosts and run setclock.
  # To disable the AIX network timeserver, set the value to ""
  # !!! update !!!
  # aix_timeserver: "19.40.76.10"
  #aix_timeserver: 

  # The NTP settings here are meant to provide basic client/server mode support.
  # Valid values for state are "present" and "absent". If the environment does
  # not use NTP then Oracle will use Cluster Time Synchronization Service to
  # provide synchronization service in the cluster. If the state is "absent",
  # the AIX xnptd service will be stopped and autostart disabled. If the state is
  # "present", the time servers in the "ntp_server" will be added to
  # /etc/ntp.conf. Note that the servers in the "ntp_servers" are the
  # DESIRED servers in the configuration. Pre-existing servers that are not in
  # the "ntp_servers" will be deleted. Changes to configuration file are made
  # only to match the desired servers specified here.
  # If /etc/ntp.conf has the desired servers already, the file will be left
  # alone. Valid values for broadcastclient are "present", "absent", and ""
  # where "" means the default which is "present".
  # Changes to broadcastclient option and ntp_servers occurs only if the state
  # is "present".
  ntp:
    # !!! update !!!
    state: absent 
    ntp_servers: 
      - '0.pool.ntp.org prefer'
      - '1.pool.ntp.org'
    broadcastclient: present

  # installp files are assumed to be on NFS through <local_mnt>/<subdir> 
  # directory mount point
  lpp_src:
    # !!! update !!!
    nfs_mount: [
      # Only one will be mounted based on oslevel (Ver.Rel.Tl) that matches
      # aix_ver_rel which is set up in init.yml. For example aix_ver_rel = 7.3.1 (AIX73TL1)
      # NOTE: 
      # 1) nfs_export should be the value as shown in showmount -e <nfs_server>
      # 2) Alternatively, if nfs_export is the path that contains the fileset,
      #    then set subdir to "".
      # 3) The "local_mnt" must be the same if there are multiple entries.
      # !!! update !!!
      # local_mnt   nfs_export                      nfs_server_host aix_ver_rel_tl
      # ---------   ----------                      -----------    -----------
      ['/lppsource', '/nim/AIX73TL1/lppsource', '9.40.16.16', '7.3.3' ]
    ]

    # !!! update !!!
    subdir: 'installp/ppc'

  # When nim mount is already exists in LPAR then there is no need of NFS server for installing filesets. 
  # The LPARs created with powervs standard images have /usr/sys/inst.images filesystem mounted. 
  # If you want to install filesets from a specific location set "use_powervs_std_nim" to true and specify the fileset location "powervs_loc"
  use_powervs_std_nim: false 
  powervs_loc: '/usr/sys/inst.images/installp/ppc' 
  # AIX required fileset
  aix_filesets: [
    'bos.adt.base',
    'bos.adt.lib',
    'bos.adt.libm',
    'bos.perf.libperfstat',
    'bos.perf.perfstat',
    'bos.perf.proctools',
    'bos.loc.utf.EN_US',
    'bos.rte.security',
    'bos.rte.bind_cmds',
    'bos.compat.libs',
    'xlC.aix61.rte',
    'xlC.rte',
    'rsct.basic.rte',
    'rsct.compat.clients.rte',
    'xlsmp.msg.EN_US.rte',
    'xlfrte.aix61',
    'openssh.base.client',
    'expect.base',
    'perl.rte',
    'Java8_64.jre',
    'dsm',                      # prereqs: perl.rte, expect.base, Java8_64.jre
  ]

  # Your environment's desired minimum requirements
  # No check will be performed if the value is 0
  # !!! update !!!
  min_cores: 2 
  min_memory: 16  # in GB


###############################################################################
config:
###############################################################################

  # This is the volume group from which Oracle Grid HOME filesystem will be
  # created.
  vgname: &oravg "oravg"

  # !!! update !!!
  ora_vg: [
    # These are local disks.
    # Oracle Grid will be installed in a filesystem under the specified VG.
    # Create volume group task will fail if no entry is in this list.
    # Valid vg_types: "normal", "big", "scalable".
    # Refer to
    # https://www.ibm.com/support/pages/aix-lvm-determining-vg-type-svgsmallbig
    # "scalable" has options to set MAX number logical volumes (num_lvs) and
    # MAX number of partitions (num_partitions), but they are not considered
    # here for the sake of simplicity. Hopefully, the basic parameters
    # are sufficient.
    # Only one VG is supported, though it's a list.
    # "clear_pvids" and "zero_disks" are optional. They must be entered
    # in straight order. If an option is not needed, enter it as ''.
    # Don't miss out the commas separator!
    # NOTE: pp_size must a power of 2 and between 1 and 1024.
    #
    # vg_name   pp_size(MB) vg_type    hdisks[,,]    options
    # ------    ----------  ---------- ----------    -------
    #dummyvg,    '64',      'big',     ['hdisk10', 'hdisk11'], 'clear_pvids',
    #dummyvg,    '64',      'big',     ['hdisk10', 'hdisk11'], 'clear_pvids', 'zero_disks'
#    *oravg,     '256',      "big",     ['hdisk1', 'hdisk2'], 'clear_pvids', 'zero_disks'
#    *oravg,     '256',      "big",     ['hdisk1', 'hdisk2'], '', 'zero_disks'
    *oravg,     '256',      "big",     ['hdisk2'], 'clear_pvids',  'zero_disks'
  ]

  fs:
    # !!! update !!!
    # The filesystem mount point under which Oracle Grid HOME directory
    # will be created. This is a string type.
    ofa_fs: &ofa_fs "/u01/app"
    # The directory ACFS will be mounted on (mount point directory)
    # This is a list type.
    acfs:   &acfs   [ *ofa_fs, "19c", *db_owner ]
    # if you want to use acfs then acfs_flag is true, if you want to use jfs then acfs_flag false
    acfs_flag: false

  # !!! update !!!
  ora_fs: [
    # Make sure the size is within the combined capacity of the disks
    # that make up the volume group.
    # mountpoint   size    volgroup
    # ----------   ----    --------
    # "dummyfs'    "50G",  'dummyvg'
    *ofa_fs,       "150G",  *oravg,
  ]

  # The following vars are of list type.
  grid_base: &grid_base [ *ofa_fs, *grid_owner ]
  grid_home: &grid_home [ *ofa_fs, "19c", *grid_owner ]
  db_base:   &db_base   [ *ofa_fs, *db_owner ]
  db_home:   &db_home   [ *acfs ]

  # Set paging space size - the final size, round up to rootvg's partition size.
  # Space is allocated from rootvg.
  # AIX install has 512MB configured by default. Set to 0 to leave the size as
  # is. NOTE: Oracle Grid requires >= 16GB to install.
  paging_space_MB: 16384 
  
  # Set the FREE SPACE of the filesystems.
  # NOTE: Oracle Grid and database install require >= 5GB /tmp free space.
  fs_sizes: [
    #mount     Free Space(valid units abbreviations: G, M, K)
    #-----     ----------------------------------------------
    "/",       "1G",
    "/usr",    "2G",
    "/var",    "1G",
    "/tmp",    "8G",
    "/admin",  "1G",
    "/opt",    "3G",
    "/home",   "512M",
  ]

  # The required RPM linux tools that will be installed/validated.
  linux:
     # After Ansible has completed Oracle RAC install, VNC is needed to
     # run ASMCA to configure/administer ASM disk groups and volumes or DBCA
     # to create databases.
     # If tightvnc-server is in the list and the RPM is installed successfully,
     # The VNC passwd files will be created for the user global.db_owner 
     # and global.db_owner.
     # Keep "tools" list as is. For additional RPM, use the "optional_tools"
     # variable. Use empty list [ ] instead of '' incase of no rpms need to be applied
     tools: [
        'unzip', 'expect', 'bash'
     ]
     # !!! update !!!
     optional_tools: [
       'less',
     ]


  grid_groups: &grid_groups [
    'oinstall', '1000',
    'dba',      '2000',
    'asmadmin', '4000',
    'oper',     '3000',
    'asmdba',   '5000',
    'hagsuser', '6000'
  ]

  # Create a disjoint set of groups for db user or use the same as grid_groups.
  db_groups: *grid_groups

  # User attributes
  # Make sure the pgroup is in either config.grid_groups or config.db_groups.
  # For grid and db users, they should be consistent with global.grid_pgroup
  # and global.db_pgroup respectively.

  users:
    root:
      uid:          "0"
      pgid:         "0"
      pgroup:       "system"
      home:         "/"
      groups:       "bin,sys,security,cron,audit,lp"
      shell:        "/usr/bin/ksh"
      capabilities: "CAP_BYPASS_RAC_VMM,CAP_PROPAGATE,CAP_NUMA_ATTACH"
      # root password won't be changed
      password:       "{{ global.root_password }}" 
      # VNC truncates password to 8 chars
      #vnc_passwd:   "vncpass"
      # Oracle Doc ID 1587357.1 recommendation
      # nofile>1024, nofiles_hard>65536, stack>10240Kb,
      # stack_hard>10240KB<32768KB, fsize=-1, cpu=-1, data=-1, rss=-1
      ulimits: >-
               fsize=-1 data=-1 stack=-1 rss=-1 core=-1 cpu=-1 nofiles=65536

    grid:
      uid:          "1100"
      #!!! update !!! should match one config.grid_groups
      pgid:         "1000"
      pgroup:       "oinstall"
      home:         "/home/grid"
      groups:       "oinstall,dba,asmadmin,oper,asmdba,hagsuser"
      shell:        "/usr/bin/ksh"
      capabilities: "CAP_BYPASS_RAC_VMM,CAP_PROPAGATE,CAP_NUMA_ATTACH"
      # !!! update !!!
      password:       "grid"
      # VNC truncates password to 8 chars
      # !!! update !!!
    #  vnc_password:   "oracle"
      ulimits: >-
               fsize=-1 data=-1 stack=-1 rss=-1 core=-1 cpu=-1 nofiles=65536

    oracle:
      uid:          "1101"
      #!!! update !!! should match one config.db_groups
      pgid:         "1000"
      pgroup:       "oinstall"
      home:         "/home/oracle"
      groups:       "oinstall,dba,asmadmin,oper,asmdba,hagsuser"
      shell:        "/usr/bin/bash"
      capabilities: "CAP_BYPASS_RAC_VMM,CAP_PROPAGATE,CAP_NUMA_ATTACH"
      # !!! update !!!
      password:       "oracle"
      # VNC truncates password to 8 chars
      # !!! update !!!
    #  vnc_password:   "oracle"
      ulimits: >-
               fsize=-1 data=-1 stack=-1 rss=-1 core=-1 cpu=-1 nofiles=65536

  # !!! update !!!
  # Specify the cluster domain if exists
  cluster_domain: "example.com"

  # !!! update !!!
  # To skip the network IP assignment via smitty set the variable networks_defined to true
  networks_defined: false 
  networks:
    # The second column "hostname" is a temporary place holder used to satisfy
    # mktcpip -h <hostname>, except that ora_pub's "hostname" is used for
    # AIX hostname, i.e. chdev -l inet0 <ora_pub's hostname>.
    # Currently, Oracle private networks (ora_pvt1/2) are assumed to be used by
    # ASM & Interconnect networks. NOTE: "host", "pub", "ora_pub", "ora_pvt1"
    # and "ora_pvt2" are variable names and should not be changed.
    # NOTE: For each "host" variable, the IP address must match the address field
    #       (4th column) as well as the "hosts" key the inventory.yml.

      #name     hostname     intf IP address    netmask         gateway_address
      #-----    ------------ ---- -----------   --------------  --------------
      # Example of 3-network, 2-node config
#    - host: "X.Y.Z.229"
#      ora_pub:  "ora1        en2 X.Y.Z.229     255.255.255.0    X.Y.Z.1"
#      ora_pvt1: "ora1-1      en3 10.10.10.1    255.255.255.0"
#      ora_pvt2: "ora1-2      en4 10.10.20.1    255.255.255.0"
#    - host: "X.Y.Z.230"
#      ora_pub:  "ora2        en2 X.Y.Z.230     255.255.255.0    X.Y.Z.1"
#      ora_pvt1: "ora2-1      en3 10.10.10.2    255.255.255.0"
#      ora_pvt2: "ora2-2      en4 10.10.20.2    255.255.255.0"

     - host: "node1"
       ora_pub:  "node1       en1 192.168.10.91 255.255.252.0"
       ora_pvt1: "node1-priv1   en2 10.10.10.93  255.255.255.0" 
       ora_pvt2: "node1-priv2   en3 10.10.20.93  255.255.255.0" 
     - host: "node2"
       ora_pub:  "node2       en1 192.168.10.92 255.255.252.0"
       ora_pvt1: "node2-priv1   en2 10.10.10.94  255.255.255.0"
       ora_pvt2: "node2-priv2   en3 10.10.20.94  255.255.255.0"


  # These are the entries that will be inserted in /etc/hosts.
  # The 3rd list element is an optional aliases. If there's no aliases,
  # enter it as "[ '' ]".
  # If '127.0.0.1' entry is present, the AIX supplied '127.0.0.1' entry
  # will be commented out.
  # NOTE: AIX 127.0.0.1 default is
  # 127.0.0.1               loopback localhost      # loopback (lo0) name/address
  # NOTE: Pre-existing entries not in the list will be deleted.
  # The convention most people used is to use short hostname in the second
  # field in /etc/hosts, and let /etc/resolv.conf append domain name as needed
  # picked up by search entry along with netsvc.conf entry "hosts = local bind4".
  # Although Oracle DocId: 1330701.1 which applies to Linux x84_64 states that
  # the localhost entry in /etc/hosts is
  # 127.0.0.1 localhost.localdomain localhost
  # but it doesn't say it applies to AIX.

  # !!! update !!!
  etc_hosts: [
      # IP address       hostname                  optional aliases
      # ----------       ------------------------  ---------------------------
      # 'x.x.x.x'        'myhost',                 [ '', ],
#      '127.0.0.1',       'localhost',              [ 'localhost.localdomain', ],

       '192.168.10.91',  'node1',                   [ 'node1.example.com',],
       '192.168.10.92',  'node2',                   [ 'node2.example.com',],

      # Private IP addresses for private network interface 1:
       '10.10.10.93',  'node1-priv1',                [ '', ],
       '10.10.10.94',  'node2-priv1',                [ '', ],

       # Private IP addresses for private network interface 2:
       '10.10.20.93',  'node1-priv2',                [ '', ],
       '10.10.20.94',  'node2-priv2',                [ '', ],
 
   ]

  # Define hdisks for various ASM disk groups 
  asmdisks:
    mode: '660'
    diskgroups:
      # These are shared disks.
      # If <prefix> is "", hdiskX will not be renamed.
      # hdiskX will be renamed to <prefix>X or <prefix>Y depending on
      # new disks numbers are included or not. If new disks numbers is
      # not included, then hdiskX is renamed to <prefix>X, otherwise <prefix>Y
      # where Y is the value associated with the corresponding X value.
      # NOTE: The final new name must not exceed 15 chars.
      # The raw disks will be chown to global.grid_owner:global.grid_pgroup.
      # If DG name is "", it is a place holder for future ASM groups, its
      # purpose to have the permissions and ownership set on these disks.
      # Regardless of whether a DG name is specified or not, the disks
      # will have permissions, ownership and device attributes set.
      # The ownership is taken from global.grid_owner and global.grid_pgroup.
      # 
      # Disks might have PVIDs and/or ASM disk group headers which will cause
      # disk group creation to fail and manual clean up is needed.
      # However, the options "clear_pvids" and "zero_disks" can clean up the
      # headers. Make sure disks can be reused before enabling these options.
      # They are used for clearing the PVIDs on the disks and to zeroing out the
      # disks respectively. Zeroing out disks ensures there is no ASM disk group
      # headers.
      #
      # The "OCRVOTE" DG name should match
      # install.grid_rsp.install.asm.diskGroup.name.
      # The "GIMR" DG name should match
      # install.grid_rsp.install.asm.gimrDG.name.
      # The "ACFS" DG name shouild match install.install.asm_acfs.group_name.
      # The Redundancy for "OCRVOTE", "GIMR", "ACFS" are place holders are
      # ignored. The redundancy of these diskgroups are set in
      # install.grid_rsp:install.asm.{diskGroup|gimDG}.redundancy and
      # install.asm_acfs.redundancy respectively.
      # If install.grid_rsp.install.configureGIMR is false, "GIMR" DG will not
      # be created.
      #
      # Supported redundancy values are NORMAL and EXTERNAL.
      #
      # DG Name  New Prefix      Redundancy  hdisk numbers       [new disk numbers] [clear_pvids] [zero_disks]
      #--------  --------------  ----------  ------------------ ------------------- ------------- ------------
      - ["OCRVOTE1","ASMOCRVOTE", "EXTERNAL",         "3 4",            "1 2",     "clear_pvids", "zero_disks"]
      - ["GIMR1",   "ASMGIMR",    "EXTERNAL",         "5",            "1",     "clear_pvids", "zero_disks"]
 #     - ["ACFS",   "ASMACFS",    "EXTERNAL",         "20 21",            "20 21",     "clear_pvids", "zero_disks"]
      - ["DATA1",  "ASMDATA",    "EXTERNAL",         "6",            "1",     "clear_pvids", "zero_disks"]
#      - ["DATA2",  "",           "EXTERNAL", "16 17 18 19",      "",                "clear_pvids", "zero_disks"]

  dsh:
    # The following entries will be inserted in /etc/environment.
    # DSH_NODE_LIST specifies the file that will be populated with
    # the RAC node names.
    - DSH_CONTEXT=DSH
    - DSH_NODE_LIST=/.wcoll
    - DSH_NODE_RCP=/usr/bin/scp
    - DSH_NODE_RSH=/usr/bin/ssh
    - DSH_NODE_OPTS=

  # !!! update !!!
#  vnc_hosts:
    # The list of hosts that will have vnc password file created
    # in ~/.vnc/vnc_passwd for global.grid_owner and global.db_owner.
    # Enter the IP addresses of the hosts.
#    - orac-rac2 
#    - A.B.C.D


###############################################################################
install:
###############################################################################
  # If you want to ignore known prechecks failures, you can the set flag "use_ignore_prechecks" to true 
  # so that the oracle installer will ignore the prechecks during installation
  use_ignore_prechecks: false
  # If you want RDBMS home on ACFS filesystem update below variables 
  asm_acfs:
    # group_name should match DG Name defined in config.asmdisks.diskgroups
    group_name:     "ACFS"
    # !!! update !!!
    # dg_redundancy supported values are: NORMAL|EXTERNAL
    dg_redundancy:  "EXTERNAL"
    vol_name:       "acfs_vol"
    # vol_redundancy valid values are: INHERIT|MIRROR|HIGH|UNPROTECTED
    # !!! update !!!
    vol_redundancy: "UNPROTECTED"
    # !!! update !!!
    vol_size_GB:     "200"

  # !!! update !!!
  ora_zips:
    # NOTE: NFS export is the directory as shown in showmount -e <nfs_server>.
    #       This should not be treated as dir for mounting.
    #       E.g. <nfs_server>:<some_directory>,
    #       because <some_directory> can be <export>/<sub_dirs>.
    #           local mnt pt, NFS export,   NFS server IP
    #           -----------   ----------    --------------
    nfs_mount: [ '/repos',     '/repos',     '9.40.16.1', ]

    # binary location can be remote|local|nfs
    # remote : Ansible Controller location 
    # local: Local location of Target Hosts
    # nfs: Network File system location

    ora_binary_location: nfs
    # If binary location is remote or local, set remote_local_mount to path of binaries 
    remote_local_mount: /stage 

    # set the value to true if we are using goldimage.
    # if value is true, then provide the goldimage location in base_subdir, grid19c_zip, and db19c_zip variable
    # if value is false, then goldimage will not be installed
    goldimage_flag: false

    # Below are the NFS,REMOTE,LOCAL binary locations
    # !!! update !!!
    opatch_subdir: "oracle"
    opatch_zips:
      v12_2_0_1_46: &v12_2_0_1_46 "p6880880_190000_AIX64-5L.zip"   
      v12_2_0_1_42: &v12_2_0_1_42 "12.2.0.1.42/opatch-12.2.0.1.42_p6880880_190000_AIX64-5L.zip"
      v12_2_0_1_41: &v12_2_0_1_41 "12.2.0.1.41/opatch-12.2.0.1.41_p6880880_210000_AIX64-5L.zip"
      v12_2_0_1_40: &v12_2_0_1_40 "12.2.0.1.40/p6880880_190000_AIX64-5L.zip"
      v12_2_0_1_39: &v12_2_0_1_39 "p6880880_190000_AIX64-5L.zip"
      v12_2_0_1_36: &v12_2_0_1_36 "12.2.0.1.36/p6880880_210000_AIX64-5L.zip"
      v12_2_0_1_32: &v12_2_0_1_32 "OPatch_12.2.0.1.32.zip"
    #  v12_2_0_1_27: &v12_2_0_1_27 "12.2.0.1.27/p6880880_122010_AIX64-5L.zip"
     # v12_2_0_1_25: &v12_2_0_1_25 "12.2.0.1.25/p6880880_122010_AIX64-5L.zip"
      # If "latest" is not set or doesn't exist, the OPATCH that comes
      # with Grid will be used.
      latest:       *v12_2_0_1_46

    # Oracle 19c base release zips
    # !!! update !!!
    base_subdir:  "oracle"
    grid19c_zip:  "V982588-01_193000_grid.zip"
    db19c_zip:    "V982583-01_193000_db.zip"
    
    # Configuration for running Oracle Cluster Verification Utility (CluVFY) on AIX systems:
    # - 'cluvfy_subdir' specifies the location where CluVFY binary is located.
    # - 'cluvfy_zip' is the name of the zip package. Download latest Cluvfy(CUV) from Oracle Support site MoS Patch#30839369
    # - If either of these variables is left empty, CluVFY execution will be skipped during the deployment. 
    cluvfy_subdir:  "oracle"
    cluvfy_zip:     "cvupack_aix_7_ppc64.zip"

    # Oracle 19c Release Update zips
    # !!! update !!!
    ru_subdir: "oracle/RU19.27"
    ora19c_ru8:  &ora19c_ru8  "RU19.8/p31305339_190000_AIX64-5L.zip"
    ora19c_ru10: &ora19c_ru10 "RU19.10/p32226239_190000_AIX64-5L_RU19.10.zip"
    ora19c_ru11: &ora19c_ru11 "RU19.11/p32545008_190000_AIX64-5L-RU19.11.zip"
    ora19c_ru12: &ora19c_ru12 "p32895426_190000_AIX64-5L_RU19.12.zip"
    ora19c_ru14: &ora19c_ru14 "p33509923_190000_AIX64-5L.zip"
    ora19c_ru17: &ora19c_ru17 "RU19.17-p34416665_190000_AIX64-5L.zip"
    ora19c_ru18: &ora19c_ru18 "p34762026_190000_AIX64-5L.zip"
    ora19c_ru24: &ora19c_ru24 "RU19.24/p36582629_190000_AIX64-5L_GI_RU19.24.zip"
    ora19c_ru25: &ora19c_ru25 "RU19.25/p36916690_190000_AIX64-5L_GI_RU19.25.zip"
    ora19c_ru26: &ora19c_ru26 "p37257886_190000_AIX64-5L_GI_RU19.26.zip"
    ora19c_ru27: &ora19c_ru27 "p37641958_190000_AIX64-5L.zip"
    # The following is the RU that will be installed, pick one from above.
    # Note, the value begins with '*', the remainder portion should match the
    # one that begins with '&'.
    ora19c_ru: *ora19c_ru27
    ru_version: 19.27


# specifying the compiler details is optional
  compiler:
    base_fileset:
      - xlCcmp.13.1.0
      - xlCcmp.13.1.0.bundle
      - xlCcmp.13.1.0.lib
      - xlCcmp.13.1.0.license
      - xlCcmp.13.1.0.ndi
      - xlccmp.13.1.0.ndi
    fp_fileset:
      - xlccmp.13.1.0
      - xlccmp.13.1.0.ndi
      - xlccmp.13.1.0.lib
    # If no xlC to install set base_dir to ""
    # If there's no TL/SP to install, set tl_dir/sp_dir to "".
    # xLC Compiler Base fileset are assumed to be in <local_mnt>/<base>
    # TL fileset in <local_mnt>/<tl>
    # SP fileset in <local_mnt>/<sp>
    # For TL and SP, ALL files in their respective subdirs will be installed.
    # hosts is a list of nodes to install xlC.
    # !!! update !!!
    hosts:    [ '' ]
    base_dir: ""
    tl_dir:   ""
    sp_dir:   ""
    fp_dir:   ""
    nfs_mount: [
      # local_mnt   nfs_export           nfs_server_host
      # --------    ----------           ----------------
      '',  '',    ''
    ]

  # !!! update !!!
  asm_sys_password: &asm_sys_password Sys_Passw0rd

  # Refer to <Grid_HOME>/install/response/gridsetup.rsp for the descriptions
  # of the following variables which follow the same namespace here.
  # There are many configuration types and the select types are "hard-coded"
  # hence the parameters are not exposed as variables here.
  # They are:
  # oracle.install.option=CRS_CONFIG
  # oracle.install.crs.config.scanType=LOCAL_SCAN
  # oracle.install.crs.config.ClusterConfiguration=STANDALONE
  # oracle.install.crs.config.configureAsExtendedCluster=false
  # oracle.install.crs.config.gpnp.configureGNS=false
  # oracle.install.crs.config.storageOption=FLEX_ASM_STORAGE

  grid_rsp:
    # Following 2 vars are list type.
    INVENTORY_LOCATION: [ *ofa_fs, 'oraInventory' ]
    ORACLE_BASE:        [ *grid_base ]
    install:
      crs:
        config:
          gpnp:
            # !!! update !!!
            scanName: rac-scan
            scanPort: 1521
        configureGIMR: true
      asm:
        OSASM: *db_asmadmingroup
        OSDBA: *db_asmdbagroup
        # !!! update !!!
        SYSASMPassword: *asm_sys_password
        monitorPassword: monitor
        emAdminPassword:
        diskGroup:
          # This is for OCR/VOTE 
          # name should match DG name in config.asmdisks.diskgroups
          name: OCRVOTE1
          AUSize: 4
          # Currently only EXTERNAL and NORMAL are allowed for redundancy
          redundancy: EXTERNAL 
        configureGIMRDataDG: true
        gimrDG:
          # name should match DG name in config.asmdisks.diskgroups
          name: GIMR1
          AUSize: 4
          # Currently only EXTERNAL and NORMAL are allowed for redundancy
          redundancy: EXTERNAL 
      config:
        emAdminUser:
        emAdminPassword: 
        # !!! update !!!
        clusterName: orac-cluster
        # !!! update !!!
        clusterNodes: node1:node1-vip,node2:node2-vip 

  # Refer to <DB_HOME>/install/response/db_install.rsp for the descriptions
  # of the variables.
  #
  db_rsp:
    INVENTORY_LOCATION: [ *ofa_fs, 'oraInventory' ]
    ORACLE_BASE:        *db_base
    db:
      # Valid values are: EE and SE2
      InstallEdition:   EE
    OSDBA_GROUP:        *db_pgroup
    # Following 4 groups default to OSDBA_GROUP if not set
    # Should be in one of config.db_groups.
    OSOPER_GROUP:       oper
    OSDGDBA_GROUP:
    OSKMDBA_GROUP:
    OSRACDBA_GROUP: 
